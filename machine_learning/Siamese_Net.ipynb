{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy.random as rng\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"./images_background/\"\n",
    "val_folder = './images_evaluation/'\n",
    "save_path = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadimgs(path,n = 0):\n",
    "    X=[]\n",
    "    y = []\n",
    "    cat_dict = {}\n",
    "    lang_dict = {}\n",
    "    curr_y = n\n",
    "    # we load every alphabet seperately so we can isolate them later\n",
    "    for alphabet in os.listdir(path):\n",
    "        print(\"loading alphabet: \" + alphabet)\n",
    "        lang_dict[alphabet] = [curr_y,None]\n",
    "        alphabet_path = os.path.join(path,alphabet)\n",
    "        # every letter/category has it's own column in the array, so  load seperately\n",
    "        for letter in os.listdir(alphabet_path):\n",
    "            cat_dict[curr_y] = (alphabet, letter)\n",
    "            category_images=[]\n",
    "            letter_path = os.path.join(alphabet_path, letter)\n",
    "            # read all the images in the current category\n",
    "            for filename in os.listdir(letter_path):\n",
    "                image_path = os.path.join(letter_path, filename)\n",
    "                image = cv2.imread(image_path,cv2.IMREAD_GRAYSCALE)\n",
    "                category_images.append(image)\n",
    "                y.append(curr_y)\n",
    "            try:\n",
    "                X.append(np.array(category_images))\n",
    "            # edge case  - last one\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                print(\"error - category_images:\", category_images)\n",
    "            curr_y += 1\n",
    "            lang_dict[alphabet][1] = curr_y - 1\n",
    "    y = np.array(y)\n",
    "    X = np.array(X)\n",
    "    return X,y,lang_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the train images into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading alphabet: Korean\n",
      "loading alphabet: Ojibwe_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: N_Ko\n",
      "loading alphabet: Bengali\n",
      "loading alphabet: Japanese_(hiragana)\n",
      "loading alphabet: Japanese_(katakana)\n",
      "loading alphabet: Early_Aramaic\n",
      "loading alphabet: Asomtavruli_(Georgian)\n",
      "loading alphabet: Balinese\n",
      "loading alphabet: Cyrillic\n",
      "loading alphabet: Tagalog\n",
      "loading alphabet: Sanskrit\n",
      "loading alphabet: Burmese_(Myanmar)\n",
      "loading alphabet: Hebrew\n",
      "loading alphabet: Syriac_(Estrangelo)\n",
      "loading alphabet: Tifinagh\n",
      "loading alphabet: Inuktitut_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Mkhedruli_(Georgian)\n",
      "loading alphabet: Greek\n",
      "loading alphabet: Latin\n",
      "loading alphabet: Futurama\n",
      "loading alphabet: Anglo-Saxon_Futhorc\n",
      "loading alphabet: Braille\n",
      "loading alphabet: Malay_(Jawi_-_Arabic)\n",
      "loading alphabet: Grantha\n",
      "loading alphabet: Arcadian\n",
      "loading alphabet: Armenian\n",
      "loading alphabet: Alphabet_of_the_Magi\n",
      "loading alphabet: Blackfoot_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Gujarati\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(964, 20, 105, 105)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y,c=loadimgs(train_folder)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the train tensors on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./pickles/train.pickle\", \"wb\") as f:\n",
    "    pickle.dump((X,c),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the validation images into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading alphabet: Glagolitic\n",
      "loading alphabet: Old_Church_Slavonic_(Cyrillic)\n",
      "loading alphabet: Aurek-Besh\n",
      "loading alphabet: Manipuri\n",
      "loading alphabet: ULOG\n",
      "loading alphabet: Atlantean\n",
      "loading alphabet: Avesta\n",
      "loading alphabet: Gurmukhi\n",
      "loading alphabet: Malayalam\n",
      "loading alphabet: Atemayar_Qelisayer\n",
      "loading alphabet: Tengwar\n",
      "loading alphabet: Sylheti\n",
      "loading alphabet: Tibetan\n",
      "loading alphabet: Ge_ez\n",
      "loading alphabet: Keble\n",
      "loading alphabet: Mongolian\n",
      "loading alphabet: Syriac_(Serto)\n",
      "loading alphabet: Kannada\n",
      "loading alphabet: Oriya\n",
      "loading alphabet: Angelic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(659, 20, 105, 105)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xval,yval,cval=loadimgs(val_folder)\n",
    "Xval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the validation tensors on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pickles/val.pickle\", \"wb\") as f:\n",
    "    pickle.dump((Xval,cval),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del X, y ,c ,Xval, yval, cval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shape, name=None, dtype = None):\n",
    "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bias(shape, name=None, dtype = None):\n",
    "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape):\n",
    "    # Define the tensors for the two input images\n",
    "    left_input = layers.Input(input_shape)\n",
    "    right_input = layers.Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(layers.Conv2D(64, (10,10), activation='relu', input_shape=input_shape,\n",
    "                   kernel_initializer=initialize_weights))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(128, (7,7), activation='relu',\n",
    "                     kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(128, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(256, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(4096, activation='sigmoid',\n",
    "                   kernel_initializer=initialize_weights,bias_initializer=initialize_bias))\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = layers.Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = layers.Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = tf.keras.models.Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 105, 105, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 105, 105, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 4096)         38947648    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 4096)         0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            4097        lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 38,951,745\n",
      "Trainable params: 38,951,745\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model((105, 105, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr = 0.00006)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training alphabets: \n",
      "\n",
      "['Korean', 'Ojibwe_(Canadian_Aboriginal_Syllabics)', 'N_Ko', 'Bengali', 'Japanese_(hiragana)', 'Japanese_(katakana)', 'Early_Aramaic', 'Asomtavruli_(Georgian)', 'Balinese', 'Cyrillic', 'Tagalog', 'Sanskrit', 'Burmese_(Myanmar)', 'Hebrew', 'Syriac_(Estrangelo)', 'Tifinagh', 'Inuktitut_(Canadian_Aboriginal_Syllabics)', 'Mkhedruli_(Georgian)', 'Greek', 'Latin', 'Futurama', 'Anglo-Saxon_Futhorc', 'Braille', 'Malay_(Jawi_-_Arabic)', 'Grantha', 'Arcadian', 'Armenian', 'Alphabet_of_the_Magi', 'Blackfoot_(Canadian_Aboriginal_Syllabics)', 'Gujarati']\n"
     ]
    }
   ],
   "source": [
    "with open(\"pickles/train.pickle\", \"rb\") as f:\n",
    "    (Xtrain, train_classes) = pickle.load(f)\n",
    "    \n",
    "print(\"Training alphabets: \\n\")\n",
    "print(list(train_classes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation alphabets:\n",
      "\n",
      "['Glagolitic', 'Old_Church_Slavonic_(Cyrillic)', 'Aurek-Besh', 'Manipuri', 'ULOG', 'Atlantean', 'Avesta', 'Gurmukhi', 'Malayalam', 'Atemayar_Qelisayer', 'Tengwar', 'Sylheti', 'Tibetan', 'Ge_ez', 'Keble', 'Mongolian', 'Syriac_(Serto)', 'Kannada', 'Oriya', 'Angelic']\n"
     ]
    }
   ],
   "source": [
    "with open(\"pickles/val.pickle\", \"rb\") as f:\n",
    "    (Xval, val_classes) = pickle.load(f)\n",
    "\n",
    "print(\"Validation alphabets:\", end=\"\\n\\n\")\n",
    "print(list(val_classes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = Xtrain.astype('float16')\n",
    "Xval = Xval.astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size,s=\"train\"):\n",
    "    \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "    if s == 'train':\n",
    "        X = Xtrain\n",
    "        categories = train_classes\n",
    "    else:\n",
    "        X = Xval\n",
    "        categories = val_classes\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "\n",
    "    # randomly sample several classes to use in the batch\n",
    "    categories = rng.choice(n_classes,size=(batch_size,),replace=False)\n",
    "    \n",
    "    # initialize 2 empty arrays for the input image batch\n",
    "    pairs=[np.zeros((batch_size, h, w,1)) for i in range(2)]\n",
    "    \n",
    "    # initialize vector for the targets\n",
    "    targets=np.zeros((batch_size,))\n",
    "    \n",
    "    # make one half of it '1's, so 2nd half of batch has same class\n",
    "    targets[batch_size//2:] = 1\n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = rng.randint(0, n_examples)\n",
    "        pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
    "        idx_2 = rng.randint(0, n_examples)\n",
    "        \n",
    "        # pick images of same class for 1st half, different for 2nd\n",
    "        if i >= batch_size // 2:\n",
    "            category_2 = category  \n",
    "        else: \n",
    "            # add a random number to the category modulo n classes to ensure 2nd image has a different category\n",
    "            category_2 = (category + rng.randint(1,n_classes)) % n_classes\n",
    "        \n",
    "        pairs[1][i,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n",
    "    \n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, s=\"train\"):\n",
    "    \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
    "    while True:\n",
    "        pairs, targets = get_batch(batch_size,s)\n",
    "        yield (pairs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oneshot_task(N, s=\"val\", language=None):\n",
    "    \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
    "    if s == 'train':\n",
    "        X = Xtrain\n",
    "        categories = train_classes\n",
    "    else:\n",
    "        X = Xval\n",
    "        categories = val_classes\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "    \n",
    "    indices = rng.randint(0, n_examples,size=(N,))\n",
    "    if language is not None: # if language is specified, select characters for that language\n",
    "        low, high = categories[language]\n",
    "        if N > high - low:\n",
    "            raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "        categories = rng.choice(range(low,high),size=(N,),replace=False)\n",
    "\n",
    "    else: # if no language specified just pick a bunch of random letters\n",
    "        categories = rng.choice(range(n_classes),size=(N,),replace=False)            \n",
    "    true_category = categories[0]\n",
    "    ex1, ex2 = rng.choice(n_examples,replace=False,size=(2,))\n",
    "    test_image = np.asarray([X[true_category,ex1,:,:]]*N).reshape(N, w, h,1)\n",
    "    support_set = X[categories,indices,:,:]\n",
    "    support_set[0,:,:] = X[true_category,ex2]\n",
    "    support_set = support_set.reshape(N, w, h,1)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "    pairs = [test_image,support_set]\n",
    "\n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_oneshot(model, N, k, s = \"val\", verbose = 0):\n",
    "    \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "    n_correct = 0\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
    "    for i in range(k):\n",
    "        inputs, targets = make_oneshot_task(N,s)\n",
    "        probs = model.predict(inputs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct+=1\n",
    "    percent_correct = (100.0 * n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "evaluate_every = 200 # interval for evaluating on one-shot tasks\n",
    "batch_size = 32\n",
    "n_iter = 20000 # No. of training iterations\n",
    "N_way = 20 # how many classes for testing one-shot tasks\n",
    "n_val = 250 # how many one-shot tasks to validate on\n",
    "best = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process!\n",
      "-------------------------------------\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 200 iterations: 2.0025741338729857 mins\n",
      "Train Loss: 0.2850838303565979\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 39.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 39.6, previous best: -1\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 400 iterations: 4.45616641441981 mins\n",
      "Train Loss: 0.3060914874076843\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 42.0, previous best: 39.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 600 iterations: 6.8983766277631124 mins\n",
      "Train Loss: 0.3244188129901886\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 53.6, previous best: 42.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 800 iterations: 9.331849014759063 mins\n",
      "Train Loss: 0.15204691886901855\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 54.0, previous best: 53.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1000 iterations: 11.780672121047974 mins\n",
      "Train Loss: 0.14925041794776917\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 56.0, previous best: 54.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1200 iterations: 14.237940673033396 mins\n",
      "Train Loss: 0.2811620831489563\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 56.4, previous best: 56.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1400 iterations: 16.7145010749499 mins\n",
      "Train Loss: 0.0944548025727272\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 63.6, previous best: 56.4\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1600 iterations: 19.162346561749775 mins\n",
      "Train Loss: 0.281715452671051\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1800 iterations: 21.612856968243918 mins\n",
      "Train Loss: 0.323363721370697\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 69.6, previous best: 63.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2000 iterations: 24.041017591953278 mins\n",
      "Train Loss: 0.20468053221702576\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 69.6, previous best: 69.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2200 iterations: 26.561936728159587 mins\n",
      "Train Loss: 0.08963898569345474\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2400 iterations: 29.03625620206197 mins\n",
      "Train Loss: 0.06612764298915863\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2600 iterations: 31.639137530326842 mins\n",
      "Train Loss: 0.12773944437503815\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 72.4, previous best: 69.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2800 iterations: 34.16876942714055 mins\n",
      "Train Loss: 0.33247992396354675\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3000 iterations: 36.92838451464971 mins\n",
      "Train Loss: 0.07871541380882263\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3200 iterations: 39.79268418550491 mins\n",
      "Train Loss: 0.11811751872301102\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 73.2, previous best: 72.4\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3400 iterations: 42.44454714457194 mins\n",
      "Train Loss: 0.303480863571167\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 73.6, previous best: 73.2\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3600 iterations: 45.174992032845815 mins\n",
      "Train Loss: 0.14400392770767212\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3800 iterations: 48.05076754490535 mins\n",
      "Train Loss: 0.043005164712667465\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 74.0, previous best: 73.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4000 iterations: 50.90120528539022 mins\n",
      "Train Loss: 0.06424358487129211\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4200 iterations: 53.76546416282654 mins\n",
      "Train Loss: 0.08315921574831009\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4400 iterations: 56.6518127600352 mins\n",
      "Train Loss: 0.18141326308250427\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4600 iterations: 59.52821795940399 mins\n",
      "Train Loss: 0.13486343622207642\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "t_start = time.time()\n",
    "for i in range(1, n_iter+1):\n",
    "    (inputs,targets) = get_batch(batch_size)\n",
    "    loss = model.train_on_batch(inputs, targets)\n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"\\n ------------- \\n\")\n",
    "        print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()-t_start)/60.0))\n",
    "        print(\"Train Loss: {0}\".format(loss)) \n",
    "        val_acc = test_oneshot(model, N_way, n_val, verbose=True)\n",
    "        model.save_weights(os.path.join(save_path, 'weights.{}.h5'.format(i)))\n",
    "        if val_acc >= best:\n",
    "            print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "            best = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model architecture\n",
    "with open('./finalModel/oneshot_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "\n",
    "#Save optimal weights\n",
    "model.save_weights('./finalModel/oneshot_weights.h5')\n",
    "\n",
    "model.save('./finalModel/SCNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(save_path, \"weights.4400.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_oneshow_task' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1bb166e492f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_oneshow_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'make_oneshow_task' is not defined"
     ]
    }
   ],
   "source": [
    "model(make_oneshow_task(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alphabet_of_the_Magi': [0, 19],\n",
       " 'Anglo-Saxon_Futhorc': [20, 48],\n",
       " 'Arcadian': [49, 74],\n",
       " 'Armenian': [75, 115],\n",
       " 'Asomtavruli_(Georgian)': [116, 155],\n",
       " 'Balinese': [156, 179],\n",
       " 'Bengali': [180, 225],\n",
       " 'Blackfoot_(Canadian_Aboriginal_Syllabics)': [226, 239],\n",
       " 'Braille': [240, 265],\n",
       " 'Burmese_(Myanmar)': [266, 299],\n",
       " 'Cyrillic': [300, 332],\n",
       " 'Early_Aramaic': [333, 354],\n",
       " 'Futurama': [355, 380],\n",
       " 'Grantha': [381, 423],\n",
       " 'Greek': [424, 447],\n",
       " 'Gujarati': [448, 495],\n",
       " 'Hebrew': [496, 517],\n",
       " 'Inuktitut_(Canadian_Aboriginal_Syllabics)': [518, 533],\n",
       " 'Japanese_(hiragana)': [534, 585],\n",
       " 'Japanese_(katakana)': [586, 632],\n",
       " 'Korean': [633, 672],\n",
       " 'Latin': [673, 698],\n",
       " 'Malay_(Jawi_-_Arabic)': [699, 738],\n",
       " 'Mkhedruli_(Georgian)': [739, 779],\n",
       " 'N_Ko': [780, 812],\n",
       " 'Ojibwe_(Canadian_Aboriginal_Syllabics)': [813, 826],\n",
       " 'Sanskrit': [827, 868],\n",
       " 'Syriac_(Estrangelo)': [869, 891],\n",
       " 'Tagalog': [892, 908],\n",
       " 'Tifinagh': [909, 963]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a77f6ea198>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Xtrain[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a78025b128>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Xtrain[20,1,:,:],cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 2 into shape (2,105,105,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-53b8232d04c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_oneshot_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Futurama\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-53b8232d04c9>\u001b[0m in \u001b[0;36mmake_oneshot_pairs\u001b[0;34m(current_image, language)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_image\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtest_image\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtest_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msupport_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msupport_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 2 into shape (2,105,105,1)"
     ]
    }
   ],
   "source": [
    "with open(\"./pickles/lang_dic.pickle\",\"rb\")as f:\n",
    "    img_dic = pickle.load(f)\n",
    "    \n",
    "     \n",
    "    #img_dic = np.stack(img_dic)\n",
    "    \n",
    "def make_oneshot_pairs(current_image,language):\n",
    "    N = len(img_dic[language])\n",
    "    test_image = np.asarray([current_image]*N)\n",
    "    test_image  = test_image.reshape(N, 105, 105,1)\n",
    "    support_set = img_dic[language].reshape(N, 105, 105,1)\n",
    "    pairs = [test_image,support_set]\n",
    "    return pairs\n",
    "\n",
    "input = make_oneshot_pairs(Xtrain[0,0,:,:],\"Futurama\") \n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "99.44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a78065d7b8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADWtJREFUeJzt3W+IZfV9x/H3p7vZWA3iv1HMrnZXWJKIkCqDNbGUoAk1NmR9oEUJ7ZIu7BPbmD+QaPtA+ixCiKYQpIsm2RYxpkaqiCSEjSH0QbfORol/NkarqW7cuCNVU9IHdcm3D+6Zdn7rDLPec/+Nvl8w3Dlnzr3ny2/cz/2e3z3zM1WFJC35nWkXIGm2GAqSGoaCpIahIKlhKEhqGAqSGoaCpMZYQiHJFUmeTvJskhvHcQ5J45FR37yUZAPwc+BjwCHgEeC6qnpqpCeSNBYbx/CaFwPPVtVzAEm+DewAVg2FM844o7Zu3TqGUiQtOXDgwCtVNbfWceMIhc3Ai8u2DwF/cOxBSXYDuwHOPfdcFhYWxlCKpCVJ/uN4jhvHnEJW2Pema5Sq2lNV81U1Pze3ZnhJmpBxhMIh4Jxl21uAl8ZwHkljMI5QeATYnmRbkk3AtcADYziPpDEY+ZxCVR1N8pfA94ENwDeq6slRn0fSeIxjopGqegh4aByvLWm8vKNRUsNQkNQwFCQ1DAVJDUNBUsNQkNQwFCQ1DAVJDUNBUsNQkNQwFCQ1DAVJDUNBUsNQkNQwFCQ1DAVJDUNBUsNQkNQwFCQ1DAVJDUNBUsNQkNQwFCQ1DAVJDUNBUsNQkNQwFCQ1DAVJDUNBUsNQkNQwFCQ1DAVJjaFDIck5SR5OcjDJk0lu6PafluQHSZ7pHk8dXbmSxq1Pp3AU+EJVfQC4BLg+yfnAjcC+qtoO7Ou2Ja0TQ4dCVR2uqp903/8XcBDYDOwA9naH7QWu6lukpMkZyZxCkq3AhcB+4KyqOgyD4ADOHMU5JE1G71BI8h7gu8Bnq+rXb+F5u5MsJFlYXFzsW4akEekVCknexSAQ7qqq+7rdLyc5u/v52cCRlZ5bVXuqar6q5ufm5vqUse4lOe4vadz6fPoQ4E7gYFV9ddmPHgB2dt/vBO4fvjxJk7axx3MvBf4MeDzJY92+vwa+DHwnyS7gBeCafiVKmqShQ6Gq/gVYrZ+9fNjXlTRdfToF9eQcgWaRtzlLahgKkhqGgqSGoSCpYShIavjpwxQM86lDVY2hEunN7BQkNewUZpwdgibNTkFSw05hgoaZS1jtOXYQGhc7BUkNO4UJGMffOIzqNe04dCw7BUkNO4UxWg9/BblSjXYP72x2CpIadgp6k2O7BzuHdxY7BUkNOwWtaa25ETuJtxc7BUkNQ0FSw8uHGTBM+z1LH3d6efH2YqcgqWGnsE71ffedZKexdC47hvXBTkFSw05hjJbeGWfxz59XOve4uwdvqV4f7BQkNewUJmC9vBuuVuc4OwhvqZ49dgqSGnYKWtMkOwg/qZg+OwVJDTsFDW2cHYQdw/TYKUhq9A6FJBuSPJrkwW57W5L9SZ5Jck+STf3L1HpSVc2X1pdRdAo3AAeXbd8C3FpV24FXgV0jOIekCekVCkm2AH8C3NFtB7gMuLc7ZC9wVZ9zaP3r0zEkmam/CH0n6Nsp3AZ8Efhtt3068FpVHe22DwGbV3pikt1JFpIsLC4u9ixD0qgMHQpJPgEcqaoDy3evcOiKbxFVtaeq5qtqfm5ubtgytI70mWewY5icPh9JXgp8MsmVwAnAyQw6h1OSbOy6hS3AS/3LlDQpQ3cKVXVTVW2pqq3AtcAPq+pTwMPA1d1hO4H7e1eptx07htk1jvsUvgR8PsmzDOYY7hzDOSSNyUjuaKyqHwE/6r5/Drh4FK+rt7+11pzQ5HlHo6SGoaCZ4N2Ps8NQkNQwFLQu+SnE+BgKkhqGgqSGoSCpYShIahgKmil+NDl9hoKkhqEgqWEoSGoYCpIahoKkhqEgqWEoSGoYCpIahoKkhqEgqWEoSGoYCpIahoKkxkiWeJdGxSXWps9OQVLDUJDUMBQkNZxT0ExwLmF22ClIatgpaKqG7RDGsY7j8dbydl9D0k5BUsNOYQzGcX28Ht+dHIf1yU5BUqNXp5DkFOAO4AKggL8AngbuAbYCvwD+tKpe7VWl3vGz83YIk9O3U/ga8L2qej/wQeAgcCOwr6q2A/u6bUnrxNChkORk4I+AOwGq6n+q6jVgB7C3O2wvcFXfIiVNTp9O4TxgEfhmkkeT3JHkJOCsqjoM0D2eOYI614Uk7/g2f9T838hNXp9Q2AhcBNxeVRcCv+EtXCok2Z1kIcnC4uJijzIkjVKfUDgEHKqq/d32vQxC4uUkZwN0j0dWenJV7amq+aqan5ub61GGpFEaOhSq6lfAi0ne1+26HHgKeADY2e3bCdzfq0JJE9X35qW/Au5Ksgl4Dvg0g6D5TpJdwAvANT3PsW4sXfs6rzC8ac4fOHcx0CsUquoxYH6FH13e53UlTY+3OY/BKN5x3k7dhu/A64u3OUtq2CnMKN9dNS12CpIahoKkhqEgqWEoSGoYCpIahoKkhqEgqWEoSGoYCpIahoKkhqEgqWEoSGoYCpIahoKkhqEgqWEoSGoYCpIahoKkhqEgqWEoSGoYCpIahoKkhqEgqWEoSGoYCpIahoKkhqEgqWEoSGoYCpIavUIhyeeSPJnkiSR3JzkhybYk+5M8k+SeJJtGVayk8Rs6FJJsBj4DzFfVBcAG4FrgFuDWqtoOvArsGkWhkiaj7+XDRuB3k2wETgQOA5cB93Y/3wtc1fMckiZo6FCoql8CXwFeYBAGrwMHgNeq6mh32CFgc98iJU1On8uHU4EdwDbgvcBJwMdXOLRWef7uJAtJFhYXF4ctQ9KI9bl8+CjwfFUtVtUbwH3Ah4FTussJgC3ASys9uar2VNV8Vc3Pzc31KEPSKPUJhReAS5KcmCTA5cBTwMPA1d0xO4H7+5UoaZL6zCnsZzCh+BPg8e619gBfAj6f5FngdODOEdQpaUI2rn3I6qrqZuDmY3Y/B1zc53UlTY93NEpqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqrBkKSb6R5EiSJ5btOy3JD5I80z2e2u1Pkr9L8mySnya5aJzFSxq94+kUvgVcccy+G4F9VbUd2NdtA3wc2N597QZuH02ZkiZlzVCoqh8D/3nM7h3A3u77vcBVy/b/Qw38K3BKkrNHVayk8Rt2TuGsqjoM0D2e2e3fDLy47LhD3b43SbI7yUKShcXFxSHLkDRqo55ozAr7aqUDq2pPVc1X1fzc3NyIy5A0rGFD4eWly4Lu8Ui3/xBwzrLjtgAvDV+epEkbNhQeAHZ23+8E7l+2/8+7TyEuAV5fusyQtD5sXOuAJHcDHwHOSHIIuBn4MvCdJLuAF4BrusMfAq4EngX+G/j0GGqWNEZrhkJVXbfKjy5f4dgCru9blKTp8Y5GSQ1DQVLDUJDUMBQkNTKYG5xyEcki8BvglWnXchzOYPbrtMbRWQ91Hm+Nv1dVa94pOBOhAJBkoarmp13HWtZDndY4OuuhzlHX6OWDpIahIKkxS6GwZ9oFHKf1UKc1js56qHOkNc7MnIKk2TBLnYKkGTAToZDkiiRPd2s73rj2M8YvyTlJHk5yMMmTSW7o9q+4PuWUa92Q5NEkD3bb25Ls72q8J8mmGajxlCT3JvlZN6YfmrWxTPK57nf9RJK7k5wwC2M56XVSpx4KSTYAX2ewvuP5wHVJzp9uVQAcBb5QVR8ALgGu7+pabX3KaboBOLhs+xbg1q7GV4FdU6mq9TXge1X1fuCDDOqdmbFMshn4DDBfVRcAG4BrmY2x/BaTXCe1qqb6BXwI+P6y7ZuAm6Zd1wp13g98DHgaOLvbdzbw9JTr2tL9R3EZ8CCD1a9eATauNL5TqvFk4Hm6Oaxl+2dmLPn/pQRPY/DXww8CfzwrYwlsBZ5Ya+yAvweuW+m44/2aeqfAW1jXcVqSbAUuBPaz+vqU03Ib8EXgt9326cBrVXW0256F8TwPWAS+2V3m3JHkJGZoLKvql8BXGKwPchh4HTjA7I3lkt7rpK5mFkLhuNd1nIYk7wG+C3y2qn497XqWS/IJ4EhVHVi+e4VDpz2eG4GLgNur6kIGt7TPwmXX/+muyXcA24D3AicxaMWPNe2xXEvv3/8shMLMruuY5F0MAuGuqrqv273a+pTTcCnwySS/AL7N4BLiNgZL6y8toDML43kIOFRV+7vtexmExCyN5UeB56tqsareAO4DPszsjeWSsa2TOguh8AiwvZvl3cRgcueBKddEkgB3Ager6qvLfrTa+pQTV1U3VdWWqtrKYNx+WFWfAh4Gru4Om2qNAFX1K+DFJO/rdl0OPMUMjSWDy4ZLkpzY/e6XapypsVxmfOukTmti55hJlCuBnwP/DvzNtOvpavpDBm3XT4HHuq8rGVyz7wOe6R5Pm3atXb0fAR7svj8P+DcGa2X+E/DuGajv94GFbjz/GTh11sYS+FvgZ8ATwD8C756FsQTuZjDP8QaDTmDXamPH4PLh692/pccZfJryls7nHY2SGrNw+SBphhgKkhqGgqSGoSCpYShIahgKkhqGgqSGoSCp8b+r/a/UHMx9DgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict( make_oneshot_pairs(Xtrain[5,2,:,:],\"Alphabet_of_the_Magi\") )\n",
    "pin = np.argmax(predictions)\n",
    "print(pin)\n",
    "acc = np.max(predictions)*100\n",
    "print(round(acc,2))\n",
    "plt.imshow(img_dic[\"Alphabet_of_the_Magi\"][pin],cmap = 'gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_kernel",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
